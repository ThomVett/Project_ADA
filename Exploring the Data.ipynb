{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "# Part 1 : Extracting the Data\n",
    "\n",
    "In this notebook we will shortly go over the work that was done to discover the overall shape of our dataset, and how we will go about to clean it and extract what is relevant for us.\n",
    "\n",
    "Starting an sql context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by working only by working with the data of one month, to understand it and so that the computations hold on our local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = sqlContext.read.format('com.databricks.spark.xml').options(rowTag=\"entity\").load('02.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the schema of th PySpark DataFrame so that we understand how it was loaded in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- links: struct (nullable = true)\n",
      " |    |-- continuation_from: string (nullable = true)\n",
      " |    |-- continuation_to: string (nullable = true)\n",
      " |    |-- first_id: string (nullable = true)\n",
      " |    |-- last_id: string (nullable = true)\n",
      " |    |-- next_id: string (nullable = true)\n",
      " |    |-- prev_id: string (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- box: string (nullable = true)\n",
      " |    |-- entity_type: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- issue_date: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- page_no: long (nullable = true)\n",
      " |    |-- publication: string (nullable = true)\n",
      " |    |-- snp: string (nullable = true)\n",
      " |    |-- suspicious_chars_count: long (nullable = true)\n",
      " |    |-- total_chars_count: long (nullable = true)\n",
      " |    |-- updated_char_count: long (nullable = true)\n",
      " |    |-- updated_word_count: long (nullable = true)\n",
      " |    |-- word_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we look at the first row of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           full_text|               links|                meta|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|7 Suisse Nouvelle...|[null,null,Ar0010...|[10 402 1262 495,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the first row as a PySpark Row type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(full_text=\"7 Suisse Nouvelles révélations dans l'affaire de corruption qui secoue le DMF 25 Sports Le mois de janvier s'est révélé agité pour certains entraîneurs de football 26 Culture Services Rencontre avec Kathryn Bigelow, 28 Décès, 29 Carnet, Mots croisés la seule femme spécialisée 30 Cinémas, 31 Télévision, dans le cinéma d'action hollywoodien 32 Météo, « 24 heures »\", links=Row(continuation_from=None, continuation_to=None, first_id='Ar00100', last_id='Ad00109', next_id='Ar00101', prev_id=None, source='103_JDG_1996_02_01_0001.PDF'), meta=Row(box='10 402 1262 495', entity_type='Article', id='Ar00100', issue_date='01/02/1996', language='French', name='Untitled Article', page_no=1, publication='JDG', snp='Ar00100S.png', suspicious_chars_count=0, total_chars_count=307, updated_char_count=307, updated_word_count=52, word_count=64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at some of the fields of the DataFrame one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"7 Suisse Nouvelles révélations dans l'affaire de corruption qui secoue le DMF 25 Sports Le mois de janvier s'est révélé agité pour certains entraîneurs de football 26 Culture Services Rencontre avec Kathryn Bigelow, 28 Décès, 29 Carnet, Mots croisés la seule femme spécialisée 30 Cinémas, 31 Télévision, dans le cinéma d'action hollywoodien 32 Météo, « 24 heures »\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.first().full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text.meta.box : 10 402 1262 495\n",
      "text.meta.snp : Ar00100S.png\n",
      "text.links.source : 103_JDG_1996_02_01_0001.PDF\n"
     ]
    }
   ],
   "source": [
    "print('text.meta.box :',text.first().meta.box)\n",
    "print('text.meta.snp :',text.first().meta.snp)\n",
    "print('text.links.source :',text.first().links.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for each article, we have a text and several other parameters.\n",
    "\n",
    "We make the first assumptions that most of these parameters will not be of real help for us so we will keep only the following parameters.\n",
    "\n",
    " - full_text\n",
    " - meta.issue_data As we want to know which day the article was published on\n",
    " - meta.suspicious character count We need that to know the number of characters given by the OCR reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------------------+\n",
      "|           full_text|issue_date|suspicious_chars_count|\n",
      "+--------------------+----------+----------------------+\n",
      "|7 Suisse Nouvelle...|01/02/1996|                     0|\n",
      "|ÉDITORIAL Le mili...|01/02/1996|                     0|\n",
      "|Bérets bleus : la...|01/02/1996|                     0|\n",
      "+--------------------+----------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textClean = text.select('full_text','meta.issue_date','meta.suspicious_chars_count')\n",
    "textClean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look if for this month we have suspicious characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|suspicious_chars_count|\n",
      "+----------------------+\n",
      "|                     0|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textClean.select('suspicious_chars_count').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that for january 1999, no suspicious characters, which was expected as the source is surely electronic and not paper version!\n",
    "\n",
    "Now that we have taken a look at the data, we begin to apply functions to the data\n",
    "\n",
    "# Part 2 : Transforming the Data\n",
    "\n",
    "We defined a small pipeline to transform each article.\n",
    "1. Separate each text into characters\n",
    "2. Put each word to lower case, remove basic stopwords (. , \"'\" etc..)\n",
    "3. Remove common words that are not useful to our analysis (le, la, de, te etc..)\n",
    "4. Count the number of times each resulting words, and how many words are in total (needed for word frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
