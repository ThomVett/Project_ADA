{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, string, re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Data\n",
    "Know that we have learned that we will not be able to process the data using a Spark pipeline, we will implement here a Python pipeline to read the files.\n",
    "\n",
    "## 1 - Extracting the paths\n",
    "\n",
    "We first have to get all the paths containing the XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GDL = 'GDL'\n",
    "JDG = 'JDG'\n",
    "listFiles = []\n",
    "\n",
    "#Go through all the files of both journals\n",
    "for root, dirs, files in os.walk(GDL): \n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "             listFiles.append(os.path.join(root, file))\n",
    "\n",
    "for root, dirs, files in os.walk(JDG): \n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "             listFiles.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the path for each file in the dataset, we can work through each of them and count words. We will use the method described in **Filtering the Data.ipynb**\n",
    "\n",
    "## 2 - Extracting the word count\n",
    "Know that we have the paths for all text files, we can read the text and then clean it.\n",
    "\n",
    "First we need the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('french'))\n",
    "stop_words.extend(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']',\n",
    "                   '{', '}', '<', '>', '...', '-', '•', '/', '%', '«', '»', 'le',\n",
    "                  'les','..','©','plus','*','``','av','p.','fr','cette','a.','v.','b.','d.',\n",
    "                   'c.','e.','f.','i.','g.','h.','i.','j.','l.','m.','n.','o.','q.','r.','q.',\n",
    "                   's.','t.','u.','v.','w.','x.','z.','y.',\"d'un\",\"d'une\",'dm',\"c'est\",\n",
    "                   \"jusqu'au\",'entre','comme','si','di','„','&',\"qu'il\",'_','fd',\"n'a\",'alors',\n",
    "                   \"s'est\",\"n'est\",'cs','dès','où',\"jusqu'à\",'déjà',\"''\",'|','£','®','+','-«',\n",
    "                   '--','.-'])\n",
    "\n",
    "stop_words.extend(['','-.','.,','(,','»,',').','-.','av.','».',\n",
    "                      '....','..','...','.....','()','(r).','—',')-«',\n",
    "                      '\".','-,','(-.',')-.','(©'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And preparing the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listPunct = list(string.punctuation)\n",
    "listPunct.remove(\"'\")\n",
    "listPunct.remove(\"-\")\n",
    "#we do not want to stick together all the words that\n",
    "#have an apostrophe (l'armée should not become larmée)\n",
    "stopDict = dict(zip(listPunct,['']*len(listPunct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a constant which tells us how many of the most frequenc words we take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numMostFreq = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can go through the list to extract the data in each file. We also set a counter to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = len(listFiles)\n",
    "allText = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at : 0.0  %\n",
      "We are at : 2.306805074971165  %\n",
      "We are at : 4.61361014994233  %\n",
      "We are at : 6.920415224913495  %\n",
      "We are at : 9.22722029988466  %\n",
      "We are at : 11.534025374855824  %\n",
      "We are at : 13.84083044982699  %\n",
      "We are at : 16.147635524798154  %\n",
      "We are at : 18.45444059976932  %\n",
      "We are at : 20.761245674740483  %\n",
      "We are at : 23.06805074971165  %\n",
      "We are at : 25.37485582468281  %\n",
      "We are at : 27.68166089965398  %\n",
      "We are at : 29.988465974625143  %\n",
      "We are at : 32.29527104959631  %\n",
      "We are at : 34.602076124567475  %\n",
      "We are at : 36.90888119953864  %\n",
      "We are at : 39.21568627450981  %\n",
      "We are at : 41.522491349480966  %\n",
      "We are at : 43.82929642445214  %\n",
      "We are at : 46.1361014994233  %\n",
      "We are at : 48.44290657439446  %\n",
      "We are at : 50.74971164936562  %\n",
      "We are at : 53.05651672433679  %\n",
      "We are at : 55.36332179930796  %\n",
      "We are at : 57.67012687427913  %\n",
      "We are at : 59.976931949250286  %\n",
      "We are at : 62.28373702422145  %\n",
      "We are at : 64.59054209919262  %\n",
      "We are at : 66.89734717416378  %\n",
      "We are at : 69.20415224913495  %\n",
      "We are at : 71.51095732410612  %\n",
      "We are at : 73.81776239907728  %\n",
      "We are at : 76.12456747404845  %\n",
      "We are at : 78.43137254901961  %\n",
      "We are at : 80.73817762399077  %\n",
      "We are at : 83.04498269896193  %\n",
      "We are at : 85.35178777393311  %\n",
      "We are at : 87.65859284890428  %\n",
      "We are at : 89.96539792387543  %\n",
      "We are at : 92.2722029988466  %\n",
      "We are at : 94.57900807381776  %\n",
      "We are at : 96.88581314878893  %\n",
      "We are at : 99.19261822376009  %\n"
     ]
    }
   ],
   "source": [
    "for idx, file in enumerate(listFiles):\n",
    "    #counter \n",
    "    counter(idx)\n",
    "    #find the date at which the article is published.\n",
    "    date = pd.to_datetime(file[4:11])\n",
    "    #Open the XML\n",
    "    f = open(file,'r')\n",
    "    soupArticle = BeautifulSoup(f,'lxml')\n",
    "    f.close()\n",
    "    \n",
    "    #Extracting the entire text from the XML\n",
    "    Article = []\n",
    "    for ft in soupArticle.find_all('full_text'):\n",
    "        #Here we lower each words and we remove the numbers from the text\n",
    "        Article.append(re.sub(r'\\d+', '', ft.text).lower())\n",
    "\n",
    "    textArticle = ' '.join(Article)  \n",
    "    \n",
    "    #Cleaning and counting\n",
    "    for k,v in stopDict.items():\n",
    "        textArticle = textArticle.replace(k,v)\n",
    "\n",
    "    dictCounter = [wordsDict for wordsDict in textArticle.split(' ') if wordsDict not in stop_words]\n",
    "    countDict = Counter(dictCounter).most_common(numMostFreq)\n",
    "    \n",
    "    #Transofming to a dataframe\n",
    "    df = pd.DataFrame(countDict).transpose()\n",
    "    df.columns = df.loc[0]\n",
    "    df.drop(0,inplace=True)\n",
    "    df.index = [date]\n",
    "    \n",
    "    allText.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = allText[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfAllText = pd.concat(allText,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAllText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fucntions for the extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def counter(i):\n",
    "    if i%100==0:\n",
    "        print('We are at :', i/total*100 ,' %')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
