{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "# Part 1 : Extracting the Data\n",
    "\n",
    "In this notebook we will shortly go over the work that was done to discover the overall shape of our dataset, and how we will go about to clean it and extract what is relevant for us.\n",
    "\n",
    "Starting an sql context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by working only by working with the data of one month, to understand it and so that the computations hold on our local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = sqlContext.read.format('com.databricks.spark.xml').options(rowTag=\"entity\").load('02.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the schema of th PySpark DataFrame so that we understand how it was loaded in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- links: struct (nullable = true)\n",
      " |    |-- continuation_from: string (nullable = true)\n",
      " |    |-- continuation_to: string (nullable = true)\n",
      " |    |-- first_id: string (nullable = true)\n",
      " |    |-- last_id: string (nullable = true)\n",
      " |    |-- next_id: string (nullable = true)\n",
      " |    |-- prev_id: string (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- box: string (nullable = true)\n",
      " |    |-- entity_type: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- issue_date: string (nullable = true)\n",
      " |    |-- language: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- page_no: long (nullable = true)\n",
      " |    |-- publication: string (nullable = true)\n",
      " |    |-- snp: string (nullable = true)\n",
      " |    |-- suspicious_chars_count: long (nullable = true)\n",
      " |    |-- total_chars_count: long (nullable = true)\n",
      " |    |-- updated_char_count: long (nullable = true)\n",
      " |    |-- updated_word_count: long (nullable = true)\n",
      " |    |-- word_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we look at the first row of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           full_text|               links|                meta|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|YOUGOSLAVIE La re...|[null,null,Ar0010...|[87 468 733 1392,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the first row as a PySpark Row type. It contains each element of each columnm we only look at the first part of the column (here the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"YOUGOSLAVIE La revanche de l'histoire S'il est un pays d'Europe où la détente, avec les années, a op\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.first()[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at some of the fields of the DataFrame one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(box='87 468 733 1392', entity_type='Article', id='Ar00100', issue_date='01/02/1989', language='French', name=\"YOUGOSLAVIE La revanche de l'histoire\", page_no=1, publication='JDG', snp='Ar0010000.png', suspicious_chars_count=0, total_chars_count=3218, updated_char_count=3193, updated_word_count=472, word_count=691)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.first().meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text.meta.box : 87 468 733 1392\n",
      "text.meta.snp : Ar0010000.png\n",
      "text.links.source : 103_JDG_1989_02_01_0001.PDF\n"
     ]
    }
   ],
   "source": [
    "print('text.meta.box :',text.first().meta.box)\n",
    "print('text.meta.snp :',text.first().meta.snp)\n",
    "print('text.links.source :',text.first().links.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for each article, we have a text and several other parameters.\n",
    "\n",
    "We make the first assumptions that most of these parameters will not be of real help for us so we will keep only the following parameters.\n",
    "\n",
    " - full_text\n",
    " - meta.issue_data As we want to know which day the article was published on\n",
    " - meta.suspicious character count We need that to know the number of characters given by the OCR reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------------------+\n",
      "|           full_text|issue_date|suspicious_chars_count|\n",
      "+--------------------+----------+----------------------+\n",
      "|YOUGOSLAVIE La re...|01/02/1989|                     0|\n",
      "|ARGENTINE Quand l...|01/02/1989|                     0|\n",
      "|ARGENTINE • Suite...|01/02/1989|                     0|\n",
      "+--------------------+----------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textClean = text.select('full_text','meta.issue_date','meta.suspicious_chars_count')\n",
    "textClean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look if for this month we have suspicious characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|suspicious_chars_count|\n",
      "+----------------------+\n",
      "|                     0|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textClean.select('suspicious_chars_count').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that for january 1999, no suspicious characters, which was expected as the source is surely electronic and not paper version!\n",
    "\n",
    "Now that we have taken a look at the data, we begin to apply functions to the data\n",
    "\n",
    "# Part 2 : Transforming the Data\n",
    "\n",
    "We defined a small pipeline to transform each article.\n",
    "1. Separate each text into characters\n",
    "2. Put each word to lower case, remove basic stopwords (. , \"'\" etc..)\n",
    "3. Remove common words that are not useful to our analysis (le, la, de, te etc..)\n",
    "4. Count the number of times each resulting words, and how many words are in total (needed for word frequency).\n",
    "\n",
    "Let's take a look at the processing steps for one article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article1 = text.select('full_text').first().full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(84, ''),\n",
       " (32, 'de'),\n",
       " (26, 'la'),\n",
       " (18, 'les'),\n",
       " (12, 'et'),\n",
       " (11, 'le'),\n",
       " (10, 'que'),\n",
       " (10, 'des'),\n",
       " (10, 'à'),\n",
       " (7, 'du')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([article1]).map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "                          .flatMap(lambda x: x.split(' '))\\\n",
    "                          .map(lambda x: (x, 1))\\\n",
    "                          .reduceByKey(lambda x,y:x+y)\\\n",
    "                          .map(lambda x:(x[1],x[0]))\\\n",
    "                          .sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we still have to do some cleaning but we have been able to count how many times a word occurs in an article.\n",
    "\n",
    "The aim now was to apply this pipeline to the whole dataset that was stored on the cluster, but we were unable to make that work due to errors that would have taken too much time to solve.\n",
    "\n",
    "We therefore decided to download the entire dataset on our personal computers and work locally only with Python, without Spark.\n",
    "The next step of our project is described in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
