{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "# Part 1 : Extracting the Data\n",
    "\n",
    "In this notebook we will shortly go over the work that was done to discover the overall shape of our dataset, and how we will go about to clean it and extract what is relevant for us.\n",
    "\n",
    "Starting an sql context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by working only by working with the data of one month, to understand it and so that the computations hold on our local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = sqlContext.read.format('com.databricks.spark.xml').options(rowTag=\"entity\").load('02.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the schema of th PySpark DataFrame so that we understand how it was loaded in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we look at the first row of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the first row as a PySpark Row type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at some of the fields of the DataFrame one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.first().full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('text.meta.box :',text.first().meta.box)\n",
    "print('text.meta.snp :',text.first().meta.snp)\n",
    "print('text.links.source :',text.first().links.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for each article, we have a text and several other parameters.\n",
    "\n",
    "We make the first assumptions that most of these parameters will not be of real help for us so we will keep only the following parameters.\n",
    "\n",
    " - full_text\n",
    " - meta.issue_data As we want to know which day the article was published on\n",
    " - meta.suspicious character count We need that to know the number of characters given by the OCR reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textClean = text.select('full_text','meta.issue_date','meta.suspicious_chars_count')\n",
    "textClean.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look if for this month we have suspicious characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textClean.select('suspicious_chars_count').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that for january 1999, no suspicious characters, which was expected as the source is surely electronic and not paper version!\n",
    "\n",
    "Now that we have taken a look at the data, we begin to apply functions to the data\n",
    "\n",
    "# Part 2 : Transforming the Data\n",
    "\n",
    "We defined a small pipeline to transform each article.\n",
    "1. Separate each text into characters\n",
    "2. Put each word to lower case, remove basic stopwords (. , \"'\" etc..)\n",
    "3. Remove common words that are not useful to our analysis (le, la, de, te etc..)\n",
    "4. Count the number of times each resulting words, and how many words are in total (needed for word frequency).\n",
    "\n",
    "Let's take a look at the processing steps for one article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article1 = te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
